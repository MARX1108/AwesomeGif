{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "involved-script",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import tensorflow_hub as hub \n",
    "from tensorflow.keras import layers \n",
    "import bert \n",
    "import re \n",
    "# re â€” Regular expression operations\n",
    "import math\n",
    "import csv\n",
    "import pandas as pd                     \n",
    "import cv2\n",
    "from PIL import Image\n",
    "from tensorflow.keras import losses\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efficient-greensboro",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125782, 2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"/Users/zimingfang/Desktop/Animated GIFs/AwesomeGif/tgif-v1.0.tsv\", sep='\\t')\n",
    "data.isnull().values.any()\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "composed-martin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "positive-warner",
   "metadata": {},
   "outputs": [],
   "source": [
    "gif_links = list(data.y.values)\n",
    "raw_tweets = list(data.x.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "instrumental-fraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gif_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "challenging-accommodation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modern-cliff",
   "metadata": {},
   "source": [
    "# Tweet Pre-Process "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "green-catch",
   "metadata": {},
   "source": [
    "## Remove Special Char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "severe-breed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definition for function for removing html tags \n",
    "TAG_RE = re.compile(r'<[^>]+>')\n",
    "\n",
    "def remove_tags(text):\n",
    "    return TAG_RE.sub('', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "roman-equipment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definition for function for remove any punctuations and special characters\n",
    "def preprocess_text(raw_tweaet):\n",
    "    # Removing html tags\n",
    "    tweet = remove_tags(raw_tweaet)\n",
    "    # Removing html tags\n",
    "    tweet = re.sub('[^a-zA-Z]', '', tweet)\n",
    "    # Removing html tags\n",
    "    tweet = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', tweet)\n",
    "    # Removing multiple spaces\n",
    "    tweet = re.sub(r'\\s+', ' ', tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "scheduled-mailing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the preprocess_text function to clean tweets list \n",
    "tweets = [] \n",
    "for tweet in raw_tweets[:50]:\n",
    "    tweets.append(preprocess_text(tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "permanent-panama",
   "metadata": {},
   "source": [
    "## Tokenizing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "editorial-protest",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "SavedModel file does not exist at: /var/folders/kk/myrp2vm96373t9hlm54kz_ym0000gn/T/tfhub_modules/03d6fb3ce1605ad9e5e9ed5346b2fb9623ef4d3d/{saved_model.pbtxt|saved_model.pb}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-ec0bf3d68de0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Create a tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mBertTokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert_tokenization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFullTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mbert_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKerasLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mvocabulary_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolved_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masset_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# .numpy(): converts a tensor object into an numpy.ndarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.4/lib/python3.6/site-packages/tensorflow_hub/keras_layer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, handle, trainable, arguments, _sentinel, tags, signature, signature_outputs_as_dict, output_key, output_shape, load_options, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_training_argument\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_has_training_argument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_hub_module_v1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_is_hub_module_v1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.4/lib/python3.6/site-packages/tensorflow_hub/keras_layer.py\u001b[0m in \u001b[0;36mload_module\u001b[0;34m(handle, tags, load_options)\u001b[0m\n\u001b[1;32m    425\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Expected before TF2.4.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0mset_load_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodule_v2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mset_load_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.4/lib/python3.6/site-packages/tensorflow_hub/module_v2.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(handle, tags, options)\u001b[0m\n\u001b[1;32m    104\u001b[0m         module_path, tags=tags, options=options)\n\u001b[1;32m    105\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m   \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_hub_module_v1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mis_hub_module_v1\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.4/lib/python3.6/site-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(export_dir, tags, options)\u001b[0m\n\u001b[1;32m    857\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mdon\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0mt\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0ma\u001b[0m \u001b[0mMetaGraph\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mSavedModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m   \"\"\"\n\u001b[0;32m--> 859\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mload_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexport_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"root\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.4/lib/python3.6/site-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36mload_internal\u001b[0;34m(export_dir, tags, options, loader_cls, filters)\u001b[0m\n\u001b[1;32m    869\u001b[0m     \u001b[0mtags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m   saved_model_proto, debug_info = (\n\u001b[0;32m--> 871\u001b[0;31m       loader_impl.parse_saved_model_with_debug_info(export_dir))\n\u001b[0m\u001b[1;32m    872\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m   if (len(saved_model_proto.meta_graphs) == 1 and\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.4/lib/python3.6/site-packages/tensorflow/python/saved_model/loader_impl.py\u001b[0m in \u001b[0;36mparse_saved_model_with_debug_info\u001b[0;34m(export_dir)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mparsed\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mMissing\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0mdebug\u001b[0m \u001b[0minfo\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mfine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m   \"\"\"\n\u001b[0;32m---> 56\u001b[0;31m   \u001b[0msaved_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parse_saved_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexport_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m   debug_info_path = os.path.join(\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.4/lib/python3.6/site-packages/tensorflow/python/saved_model/loader_impl.py\u001b[0m in \u001b[0;36mparse_saved_model\u001b[0;34m(export_dir)\u001b[0m\n\u001b[1;32m    112\u001b[0m                   (export_dir,\n\u001b[1;32m    113\u001b[0m                    \u001b[0mconstants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSAVED_MODEL_FILENAME_PBTXT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m                    constants.SAVED_MODEL_FILENAME_PB))\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: SavedModel file does not exist at: /var/folders/kk/myrp2vm96373t9hlm54kz_ym0000gn/T/tfhub_modules/03d6fb3ce1605ad9e5e9ed5346b2fb9623ef4d3d/{saved_model.pbtxt|saved_model.pb}"
     ]
    }
   ],
   "source": [
    "# Create a tokenizer \n",
    "BertTokenizer = bert.bert_tokenization.FullTokenizer\n",
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
    "                            trainable=False)\n",
    "vocabulary_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "# .numpy(): converts a tensor object into an numpy.ndarray\n",
    "to_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = BertTokenizer(vocabulary_file, to_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modular-imagination",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition for function for convert tweet to ids \n",
    "def tokenize_tweets(text_tweets):\n",
    "    return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "difficult-proportion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the tokenize_tweets on tweets \n",
    "tokenized_tweets = [tokenize_tweets(tweet) for tweet in tweets]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silver-vector",
   "metadata": {},
   "source": [
    "# GIF Pre-Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norman-dependence",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "# Requests is an elegant and simple HTTP library for Python\n",
    "import os \n",
    "# os â€” Miscellaneous operating system interfacesÂ¶\n",
    "os.chdir('/Users/zimingfang/Desktop/Animated GIFs/AwesomeGif/gifs')\n",
    "# !pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reported-monte",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gif_downloader(image_urls, status=[], filenames = []):\n",
    "    \n",
    "    for index, img in enumerate(image_urls):\n",
    "        # We can split the file based upon / and extract the last split within the python list below:\n",
    "        file_name = img.split('/')[-1]\n",
    "        #print(\"fThis is the file name: {file_name}\")\n",
    "        filenames.append(file_name) \n",
    "        # Now let's send a request to the image URL:\n",
    "        r = requests.get(img, stream=True)\n",
    "        # We can check that the status code is 200 before doing anything else:\n",
    "        if r.status_code == 200:\n",
    "            # This command below will allow us to write the data to a file as binary:\n",
    "            with open(file_name, 'wb') as f:\n",
    "                for chunk in r:\n",
    "                    f.write(chunk)\n",
    "            status.append(True)\n",
    "        else:\n",
    "            # We will write all of the images back to the broken_images list:\n",
    "            status.append(False)\n",
    "    return filenames, status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frequent-relay",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_gif(image_path, sess):\n",
    "#     image = tf.io.read_file(image_file)\n",
    "#     image = tf.io.decode_gif(image)\n",
    "#     return sess.run(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subjective-saver",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_status = []\n",
    "filenames = []\n",
    "filenames, downlod_status = gif_downloader(gif_links[:50], status=download_status)\n",
    "# if any gif was not downloaded successfully \n",
    "print(any(status == False for status in download_status))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "terminal-puppy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "creative-stake",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_fps(PIL_Image_object):\n",
    "    \"\"\" Returns the average framerate of a PIL Image object \"\"\"\n",
    "    PIL_Image_object.seek(0)\n",
    "    frames = duration = 0\n",
    "    while True:\n",
    "        try:\n",
    "            frames += 1\n",
    "            duration += PIL_Image_object.info['duration']\n",
    "            PIL_Image_object.seek(PIL_Image_object.tell() + 1)\n",
    "        except EOFError:\n",
    "            return frames / duration * 1000\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "military-rogers",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In GIF files, each frame has its own duration. So there is no general fps for a GIF file. \n",
    "framerates = []\n",
    "for gif_file_path in filenames[:50]: \n",
    "#     cap=cv2.VideoCapture(\"gif_file_path\")\n",
    "#     fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    gif_obj = Image.open(gif_file_path)\n",
    "    framerates.append(get_avg_fps(gif_obj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rural-probe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# framerates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changing-portfolio",
   "metadata": {},
   "source": [
    "# Prerparing Data For Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatty-status",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = tokenized_tweets\n",
    "y_train = framerates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educated-administration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "therapeutic-polls",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_with_len = [[tweet, y_train[i], len(tweet)]\n",
    "                 for i, tweet in enumerate(x_train)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indonesian-finance",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort the data by tweet length \n",
    "tweets_with_len.sort(key=lambda x: x[2])\n",
    "#remove the tweet length attribute from dataset \n",
    "sorted_tweets_labels = [(tweet_lab[0], tweet_lab[1]) for tweet_lab in tweets_with_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loaded-parker",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted_tweets_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "median-thong",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the sorted dataset into a TensorFlow 2.0-compliant input dataset shape\n",
    "processed_dataset = tf.data.Dataset.from_generator(lambda: sorted_tweets_labels, output_types=(tf.int32, tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "white-vietnamese",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 10 \n",
    "batched_dataset = processed_dataset.padded_batch(BATCH_SIZE, padded_shapes=((None, ), ()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "whole-probe",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_BATCHES = math.ceil(len(sorted_tweets_labels) / BATCH_SIZE)\n",
    "# TEST_BATCHES = TOTAL_BATCHES // 10\n",
    "TEST_BATCHES = 1\n",
    "batched_dataset.shuffle(TOTAL_BATCHES)\n",
    "test_data = batched_dataset.take(TEST_BATCHES)\n",
    "train_data = batched_dataset.skip(TEST_BATCHES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alone-freeware",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(iter(batched_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bronze-turkish",
   "metadata": {},
   "source": [
    "# Creating the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "express-better",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_LENGTH = len(tokenizer.vocab)\n",
    "EMB_DIM = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cognitive-recruitment",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GlobalAveragePooling1D layer returns a fixed-length output vector for each example by averaging over the sequence dimension.\n",
    "#allows the model to handle input of variable length, in the simplest way possible\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Embedding(VOCAB_LENGTH + 1, EMB_DIM),\n",
    "    layers.Dropout(0.2),    \n",
    "    layers.GlobalAveragePooling1D(), \n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(1, activation='linear') #set activation to linear for regression\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handmade-selling",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "history = model.fit(\n",
    "    train_data,\n",
    "    validation_data=test_data,\n",
    "    epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olive-buffer",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordinary-stationery",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scattergl(y=history.history['loss'],\n",
    "                    name='Train'))\n",
    "\n",
    "fig.add_trace(go.Scattergl(y=history.history['val_loss'],\n",
    "                     name='Valid'))\n",
    "\n",
    "fig.update_layout(height=500, width=700,\n",
    "                  xaxis_title='Epoch',\n",
    "                  yaxis_title='Loss')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "editorial-surge",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(test_data)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painted-resistance",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
