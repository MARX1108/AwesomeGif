{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'youtube_dl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5fe3c983b520>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0myoutube_dl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'youtube_dl'"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals\n",
    "import json \n",
    "import requests\n",
    "import pandas \n",
    "import os\n",
    "import youtube_dl\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load json file\n",
    "def loadKeys(key_file:str):\n",
    "    with open(key_file) as f:\n",
    "        key_dict = json.load(f)\n",
    "    return key_dict['api_key'], key_dict['api_secret'], key_dict['bearer_token'], key_dict['token'], key_dict['token_secret']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a url for the api to search from\n",
    "def createFirstSearchURL(wordSearch:str):\n",
    "    query = '(%23{}) -is:retweet has:media -has:videos lang:en  -is:retweet'.format(wordSearch)\n",
    "    # Tweet fields are adjustable.\n",
    "    # Options include:\n",
    "    # attachments, author_id, context_annotations,\n",
    "    # conversation_id, created_at, entities, geo, id,\n",
    "    # in_reply_to_user_id, lang, non_public_metrics, organic_metrics,\n",
    "    # possibly_sensitive, promoted_metrics, public_metrics, referenced_tweets,\n",
    "    # source, text, and withheld\n",
    "    tweet_fields = \"media.fields=type,url\"\n",
    "    expansions = \"expansions=attachments.media_keys\"\n",
    "    max_results = \"max_results=100\"\n",
    "    url = \"https://api.twitter.com/2/tweets/search/recent?query={}&{}&{}&{}\".format(\n",
    "        query, tweet_fields, expansions, max_results\n",
    "    )\n",
    "    return url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a url for the api to search from\n",
    "def createSearchURL(wordSearch:str, token:str):\n",
    "    query = '(%23{}) -is:retweet has:media -has:videos lang:en  -is:retweet'.format(wordSearch)\n",
    "    # Tweet fields are adjustable.\n",
    "    # Options include:\n",
    "    # attachments, author_id, context_annotations,\n",
    "    # conversation_id, created_at, entities, geo, id,\n",
    "    # in_reply_to_user_id, lang, non_public_metrics, organic_metrics,\n",
    "    # possibly_sensitive, promoted_metrics, public_metrics, referenced_tweets,\n",
    "    # source, text, and withheld\n",
    "    nextToken = \"next_token={}\".format(token)\n",
    "    tweet_fields = \"media.fields=type,url\"\n",
    "    expansions = \"expansions=attachments.media_keys\"\n",
    "    max_results = \"max_results=100\"\n",
    "    url = \"https://api.twitter.com/2/tweets/search/recent?query={}&{}&{}&{}&{}\".format(\n",
    "        query, tweet_fields, expansions, max_results, nextToken\n",
    "    )\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a url for the api to search from\n",
    "def getSearchRequestCount(headers:str):\n",
    "    url = \"https://api.twitter.com/1.1/application/rate_limit_status.json?\"\n",
    "    data = connect_to_endpoint(url, headers)\n",
    "    return int(data['resources']['search']['/search/tweets']['remaining'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating authorization \n",
    "def create_headers(bearer_token):\n",
    "    headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
    "    return headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connectign to the url api with the token headers\n",
    "def connect_to_endpoint(url, headers):\n",
    "    response = requests.request(\"GET\", url, headers=headers)\n",
    "    print(response.status_code)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating json file from a response file and filename\n",
    "def create_json(filename, json_response):\n",
    "    with open(filename, \"w\") as write_file:\n",
    "        json.dump(json_response, write_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading up data of a json file\n",
    "def load_json(filename):\n",
    "    file = open(filename)\n",
    "    data = json.load(file)\n",
    "    file.close()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading gif file with given url \n",
    "def downloadGIF(url, filename):\n",
    "    try:\n",
    "        ydl_opts = {\n",
    "            'outtmpl': 'gifs/{}.mp4'.format(filename)\n",
    "        }\n",
    "        with youtube_dl.YoutubeDL(ydl_opts) as ydl:\n",
    "            ydl.download([url])\n",
    "        return True\n",
    "    except:\n",
    "        print(\"Could Not Download File\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class TweetData holds the mediaKey, TweetURL, Text, TweetID for one data object in a json file\n",
    "class TweetData:\n",
    "    def __init__(self, jsonData):\n",
    "        # Media Key Of Tweet\n",
    "        mediaKey = jsonData['attachments']['media_keys'][0]\n",
    "        # Splitting the text into the text and url\n",
    "        tempTextArray = jsonData['text'].rpartition(\"https://\")\n",
    "        # Creating URL\n",
    "        tweetURL = tempTextArray[1] + tempTextArray[2]\n",
    "        # Creating tweet text\n",
    "        text = tempTextArray[0]\n",
    "        # Setting mediaKey, URL, and text\n",
    "        self.mediaKey = mediaKey\n",
    "        self.tweetURL = tweetURL\n",
    "        self.text = text\n",
    "        self.tweetID = jsonData['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CSVandGIFS \n",
    "def createDFandGIFS(data:dict, finalTweetDF:pandas.DataFrame):\n",
    "    # Headers for the data\n",
    "    headers = [\"Tweet ID\", \"Media Key\", \"Tweet URL\", \"Tweet Text\", \"GIF Title\"]\n",
    "    # Initializing 2d array that will contain all of the data\n",
    "    tweetData = np.array([headers])   \n",
    "    # Iterate over all of the collected tweets\n",
    "    for temp in data['data']:\n",
    "        \n",
    "        try:# Creating a tweet obejct that holds the data\n",
    "            tempTweet = TweetData(temp)\n",
    "            # Check if the current media tweet contains a gif\n",
    "            if \"16_\" in tempTweet.mediaKey: \n",
    "                # Create File Name\n",
    "                curGifTitle =  \"{}\".format(tempTweet.mediaKey)\n",
    "                # Try to download new file, if an error occurs and cant download gif, the file is not added\n",
    "                if downloadGIF(tempTweet.tweetURL, curGifTitle):\n",
    "                    # If Downloading causes no error we add this twitter obeject to our array \n",
    "                    # Create a numpy row to add to data\n",
    "                    curRow = np.array([[\n",
    "                        tempTweet.tweetID,\n",
    "                        tempTweet.mediaKey,\n",
    "                        tempTweet.tweetURL,\n",
    "                        tempTweet.text,\n",
    "                        curGifTitle + \".mp4\"\n",
    "                    ]])\n",
    "                    # Adding indivisual data to whole data\n",
    "                    tweetData = np.append(tweetData, curRow, axis=0)   \n",
    "        except:\n",
    "            print(\"Error with tweet\")\n",
    "    #Create a pandas file of the \n",
    "    tweetDF = pandas.DataFrame(data = tweetData[1:,:],  columns=tweetData[0])\n",
    "    finalTweetDF = finalTweetDF.append(tweetDF, ignore_index=True)\n",
    "\n",
    "    return finalTweetDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Main Scraper function \n",
    "def scraper(query:str , bearer_token:str , csvFileName:str, dataFileName:str):\n",
    "    # Loading current csv file into a pandas data frame\n",
    "    finalTwitterDF = pandas.read_csv(csvFileName)\n",
    "    # Headers for the endpoint connection\n",
    "    headers = create_headers(bearer_token)\n",
    "    # Instantiating capCounter\n",
    "    capCounter = 450000\n",
    "    # Gets the total ammount of searches allowed with in the current 15 minute time frame. default = 450\n",
    "    searchLimit = getSearchRequestCount(headers)\n",
    "    firstSearch = True\n",
    "    # Looping through twitter api and ensuring rate limits have not been exceeded\n",
    "    while True:\n",
    "        # Creating twitter api url on first and subsequent tries\n",
    "        if firstSearch:\n",
    "            url = createFirstSearchURL(query)\n",
    "            firstSearch = False \n",
    "        else:\n",
    "            if \"next_token\" in data[\"meta\"].keys(): \n",
    "                nextToken = data[\"meta\"][\"next_token\"] \n",
    "            else:\n",
    "                return finalTwitterDF\n",
    "            url = createSearchURL(query, nextToken) #<------ edit\n",
    "        # Data collected from twitter api\n",
    "        data = connect_to_endpoint(url, headers)\n",
    "        # Saving previously used data as json file \n",
    "        create_json(dataFileName, data)\n",
    "        # Updates df with newly  collected data\n",
    "        finalTwitterDF = createDFandGIFS(data, finalTwitterDF)\n",
    "        # Subtract one from the seachres\n",
    "        searchLimit -= 1\n",
    "        capCounter -= 100\n",
    "        finalTwitterDF.to_csv(CSV_FILE_NAME, index=False)\n",
    "        if capCounter < 100:\n",
    "            return finalTwitterDF\n",
    "        if searchLimit ==  0:\n",
    "            time.sleep(60 * 15) # wait for 15 minutes so that we can reset the rate limits\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# Creating keys for the twitter api\n",
    "api_key, api_secret, bear_token, token, token_secret = loadKeys(\"keys.json\")\n",
    "bearer_token = bear_token\n",
    "# CSV File Name\n",
    "CSV_FILE_NAME = \"twitterData.csv\"\n",
    "DATA_FILE_NAME = \"data_file.json\"\n",
    "QUERY = \"sad\"\n",
    "df = scraper(QUERY, bear_token, CSV_FILE_NAME, DATA_FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nextToken = data[\"meta\"][\"next_token\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
