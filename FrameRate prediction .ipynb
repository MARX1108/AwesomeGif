{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "retained-receiver",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import tensorflow_hub as hub \n",
    "from tensorflow.keras import layers \n",
    "import bert \n",
    "import re \n",
    "# re — Regular expression operations\n",
    "import math\n",
    "import csv\n",
    "import pandas as pd                     \n",
    "import cv2\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sunrise-value",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125782, 2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"/Users/zimingfang/Desktop/Animated GIFs/AwesomeGif/tgif-v1.0.tsv\", sep='\\t')\n",
    "data.isnull().values.any()\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "collectible-sleep",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "complimentary-offense",
   "metadata": {},
   "outputs": [],
   "source": [
    "gif_links = list(data.y.values)\n",
    "raw_tweets = list(data.x.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "alike-terminology",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gif_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "checked-technique",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rural-madrid",
   "metadata": {},
   "source": [
    "# Tweet Pre-Process "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "veterinary-quest",
   "metadata": {},
   "source": [
    "## Remove Special Char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "honest-prairie",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definition for function for removing html tags \n",
    "TAG_RE = re.compile(r'<[^>]+>')\n",
    "\n",
    "def remove_tags(text):\n",
    "    return TAG_RE.sub('', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dying-macro",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definition for function for remove any punctuations and special characters\n",
    "def preprocess_text(raw_tweaet):\n",
    "    # Removing html tags\n",
    "    tweet = remove_tags(raw_tweaet)\n",
    "    # Removing html tags\n",
    "    tweet = re.sub('[^a-zA-Z]', '', tweet)\n",
    "    # Removing html tags\n",
    "    tweet = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', tweet)\n",
    "    # Removing multiple spaces\n",
    "    tweet = re.sub(r'\\s+', ' ', tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "convertible-result",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the preprocess_text function to clean tweets list \n",
    "tweets = [] \n",
    "for tweet in raw_tweets[:50]:\n",
    "    tweets.append(preprocess_text(tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "commercial-ranch",
   "metadata": {},
   "source": [
    "## Tokenizing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "respiratory-polymer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tokenizer \n",
    "BertTokenizer = bert.bert_tokenization.FullTokenizer\n",
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\", trainable=False)\n",
    "vocabulary_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "# .numpy(): converts a tensor object into an numpy.ndarray\n",
    "to_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = BertTokenizer(vocabulary_file, to_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "close-laundry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition for function for convert tweet to ids \n",
    "def tokenize_tweets(text_tweets):\n",
    "    return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "portuguese-puzzle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the tokenize_tweets on tweets \n",
    "tokenized_tweets = [tokenize_tweets(tweet) for tweet in tweets]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mental-cuisine",
   "metadata": {},
   "source": [
    "# GIF Pre-Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "technical-ordinance",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "# Requests is an elegant and simple HTTP library for Python\n",
    "import os \n",
    "# os — Miscellaneous operating system interfaces¶\n",
    "os.chdir('/Users/zimingfang/Desktop/Animated GIFs/AwesomeGif/gifs')\n",
    "# !pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "behind-grove",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gif_downloader(image_urls, status=[], filenames = []):\n",
    "    \n",
    "    for index, img in enumerate(image_urls):\n",
    "        # We can split the file based upon / and extract the last split within the python list below:\n",
    "        file_name = img.split('/')[-1]\n",
    "        #print(\"fThis is the file name: {file_name}\")\n",
    "        filenames.append(file_name) \n",
    "        # Now let's send a request to the image URL:\n",
    "        r = requests.get(img, stream=True)\n",
    "        # We can check that the status code is 200 before doing anything else:\n",
    "        if r.status_code == 200:\n",
    "            # This command below will allow us to write the data to a file as binary:\n",
    "            with open(file_name, 'wb') as f:\n",
    "                for chunk in r:\n",
    "                    f.write(chunk)\n",
    "            status.append(True)\n",
    "        else:\n",
    "            # We will write all of the images back to the broken_images list:\n",
    "            status.append(False)\n",
    "    return filenames, status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "buried-nashville",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_gif(image_path, sess):\n",
    "#     image = tf.io.read_file(image_file)\n",
    "#     image = tf.io.decode_gif(image)\n",
    "#     return sess.run(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "incomplete-precipitation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "download_status = []\n",
    "filenames = []\n",
    "filenames, download_status = gif_downloader(gif_links[:50], status=download_status)\n",
    "# if any gif was not downloaded successfully \n",
    "print(any(status == False for status in download_status))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ahead-loading",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cardiovascular-loading",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_fps(PIL_Image_object):\n",
    "    \"\"\" Returns the average framerate of a PIL Image object \"\"\"\n",
    "    PIL_Image_object.seek(0)\n",
    "    frames = duration = 0\n",
    "    while True:\n",
    "        try:\n",
    "            frames += 1\n",
    "            duration += PIL_Image_object.info['duration']\n",
    "            PIL_Image_object.seek(PIL_Image_object.tell() + 1)\n",
    "        except EOFError:\n",
    "            return frames / duration * 1000\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "reduced-travel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In GIF files, each frame has its own duration. So there is no general fps for a GIF file. \n",
    "framerates = []\n",
    "for gif_file_path in filenames: \n",
    "#     cap=cv2.VideoCapture(\"gif_file_path\")\n",
    "#     fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    gif_obj = Image.open(gif_file_path)\n",
    "    framerates.append(get_avg_fps(gif_obj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "widespread-google",
   "metadata": {},
   "outputs": [],
   "source": [
    "# framerates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rubber-machine",
   "metadata": {},
   "source": [
    "# Prerparing Data For Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "regulated-interface",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = tokenized_tweets\n",
    "y_train = framerates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "interracial-return",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "auburn-groove",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_with_len = [[tweet, y_train[i], len(tweet)]\n",
    "                 for i, tweet in enumerate(x_train)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "level-first",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort the data by tweet length \n",
    "tweets_with_len.sort(key=lambda x: x[2])\n",
    "#remove the tweet length attribute from dataset \n",
    "sorted_tweets_labels = [(tweet_lab[0], tweet_lab[1]) for tweet_lab in tweets_with_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "blind-sheriff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted_tweets_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "private-miller",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the sorted dataset into a TensorFlow 2.0-compliant input dataset shape\n",
    "processed_dataset = tf.data.Dataset.from_generator(lambda: sorted_tweets_labels, output_types=(tf.int32, tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "thousand-flood",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 10 \n",
    "batched_dataset = processed_dataset.padded_batch(BATCH_SIZE, padded_shapes=((None, ), ()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "structural-cleanup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(iter(batched_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "advanced-beaver",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_BATCHES = math.ceil(len(sorted_tweets_labels) / BATCH_SIZE)\n",
    "TEST_BATCHES = 1 \n",
    "batched_dataset.shuffle(TOTAL_BATCHES)\n",
    "test_data = batched_dataset.take(TEST_BATCHES)\n",
    "train_data = batched_dataset.skip(TEST_BATCHES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unsigned-crowd",
   "metadata": {},
   "source": [
    "# Creating the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "outstanding-badge",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TEXT_MODEL(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 vocabulary_size,\n",
    "                 embedding_dimensions=128,\n",
    "                 cnn_filters=50,\n",
    "                 dnn_units=512,\n",
    "                 model_output_classes=2,\n",
    "                 dropout_rate=0.1,\n",
    "                 training=False,\n",
    "                 name=\"text_model\"):\n",
    "        super(TEXT_MODEL, self).__init__(name=name)\n",
    "        \n",
    "        self.embedding = layers.Embedding(vocabulary_size,\n",
    "                                          embedding_dimensions)\n",
    "        self.cnn_layer1 = layers.Conv1D(filters=cnn_filters,\n",
    "                                        kernel_size=2,\n",
    "                                        padding=\"valid\",\n",
    "                                        activation=\"relu\")\n",
    "        self.cnn_layer2 = layers.Conv1D(filters=cnn_filters,\n",
    "                                        kernel_size=3,\n",
    "                                        padding=\"valid\",\n",
    "                                        activation=\"relu\")\n",
    "        self.cnn_layer3 = layers.Conv1D(filters=cnn_filters,\n",
    "                                        kernel_size=4,\n",
    "                                        padding=\"valid\",\n",
    "                                        activation=\"relu\")\n",
    "        self.pool = layers.GlobalMaxPool1D()\n",
    "        \n",
    "        self.dense_1 = layers.Dense(units=dnn_units, activation=\"relu\")\n",
    "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
    "        if model_output_classes == 2:\n",
    "            self.last_dense = layers.Dense(units=1,\n",
    "                                           activation=\"sigmoid\")\n",
    "        else:\n",
    "            self.last_dense = layers.Dense(units=model_output_classes,\n",
    "                                           activation=\"softmax\")\n",
    "    \n",
    "    def call(self, inputs, training):\n",
    "        l = self.embedding(inputs)\n",
    "        l_1 = self.cnn_layer1(l) \n",
    "        l_1 = self.pool(l_1) \n",
    "        l_2 = self.cnn_layer2(l) \n",
    "        l_2 = self.pool(l_2)\n",
    "        l_3 = self.cnn_layer3(l)\n",
    "        l_3 = self.pool(l_3) \n",
    "        \n",
    "        concatenated = tf.concat([l_1, l_2, l_3], axis=-1) # (batch_size, 3 * cnn_filters)\n",
    "        concatenated = self.dense_1(concatenated)\n",
    "        concatenated = self.dropout(concatenated, training)\n",
    "        model_output = self.last_dense(concatenated)\n",
    "        \n",
    "        return model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "blind-essex",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_LENGTH = len(tokenizer.vocab)\n",
    "EMB_DIM = 200\n",
    "CNN_FILTERS = 100\n",
    "DNN_UNITS = 256\n",
    "OUTPUT_CLASSES = 2\n",
    "\n",
    "DROPOUT_RATE = 0.2\n",
    "\n",
    "NB_EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "compound-manual",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_model = TEXT_MODEL(vocabulary_size=VOCAB_LENGTH,\n",
    "                        embedding_dimensions=EMB_DIM,\n",
    "                        cnn_filters=CNN_FILTERS,\n",
    "                        dnn_units=DNN_UNITS,\n",
    "                        model_output_classes=OUTPUT_CLASSES,\n",
    "                        dropout_rate=DROPOUT_RATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "everyday-reservoir",
   "metadata": {},
   "outputs": [],
   "source": [
    "if OUTPUT_CLASSES == 2:\n",
    "    text_model.compile(loss=\"binary_crossentropy\",\n",
    "                       optimizer=\"adam\",\n",
    "                       metrics=[\"accuracy\"])\n",
    "else:\n",
    "    text_model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                       optimizer=\"adam\",\n",
    "                       metrics=[\"sparse_categorical_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "neutral-windows",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "4/4 [==============================] - 1s 61ms/step - loss: -0.0923 - accuracy: 0.0000e+00\n",
      "Epoch 2/5\n",
      "4/4 [==============================] - 0s 50ms/step - loss: -9.7960 - accuracy: 0.0000e+00\n",
      "Epoch 3/5\n",
      "4/4 [==============================] - 0s 53ms/step - loss: -20.3690 - accuracy: 0.0000e+00\n",
      "Epoch 4/5\n",
      "4/4 [==============================] - 0s 46ms/step - loss: -35.6190 - accuracy: 0.0000e+00\n",
      "Epoch 5/5\n",
      "4/4 [==============================] - 0s 42ms/step - loss: -57.5604 - accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x10d59b1d0>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_model.fit(train_data, epochs=NB_EPOCHS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "congressional-netherlands",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 182ms/step - loss: -52.6968 - accuracy: 0.0000e+00\n",
      "[-52.69684982299805, 0.0]\n"
     ]
    }
   ],
   "source": [
    "results = text_model.evaluate(test_data)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executive-prefix",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
