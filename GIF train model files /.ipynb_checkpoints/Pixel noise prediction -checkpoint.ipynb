{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "smart-witch",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras import layers\n",
    "import bert\n",
    "import re\n",
    "import math\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "moved-discount",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow\n",
    "# !pip install tensorflow_hub\n",
    "# !pip install bert-for-tf2\n",
    "# !pip install sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generic-correction",
   "metadata": {},
   "source": [
    "# Data Import & Preprocssing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "another-compensation",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'tgif-v1.0.tsv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-62b63a6cfb3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#import dataset x: tweets y:gif\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tgif-v1.0.tsv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.4/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.4/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.4/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.4/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.4/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'tgif-v1.0.tsv'"
     ]
    }
   ],
   "source": [
    "#import dataset x: tweets y:gif\n",
    "df=pd.read_csv(\"/Users/zimingfang/Desktop/Animated GIFs/AwesomeGif/tgif-v1.0.tsv\", sep='\\t')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "productive-hepatitis",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().values.any() #check if there's any null value\n",
    "df.shape #check the shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specialized-jenny",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocess(tweet):\n",
    "    # Remove punctuations and numbers\n",
    "    tweet = re.sub('[^a-zA-Z]', ' ', tweet)\n",
    "    \n",
    "    # Single character removal\n",
    "    tweet = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', tweet)\n",
    "\n",
    "    # Removing multiple spaces\n",
    "    tweet = re.sub(r'\\s+', ' ', tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protective-burning",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "for t in list(df['x']):\n",
    "    x.append(text_preprocess(t))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "running-sculpture",
   "metadata": {},
   "source": [
    "# Tokenize Text to Vector W/ Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premium-airport",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an object of the FullTokenizer class\n",
    "BertTokenizer = bert.bert_tokenization.FullTokenizer\n",
    "\n",
    "#create a BERT embedding layer by importing the BERT model from hub.KerasLayer \n",
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
    "                            trainable=False)\n",
    "# create a BERT vocabulary file in the form a numpy array\n",
    "vocabulary_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "\n",
    "to_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = BertTokenizer(vocabulary_file, to_lower_case)\n",
    "\n",
    "#ref: https://stackabuse.com/text-classification-with-bert-tokenizer-and-tf-2-0-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spectacular-stopping",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.tokenize(x[1])\n",
    "#ids of the tokens\n",
    "tokenizer.convert_tokens_to_ids(tokenizer.tokenize(x[1])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documentary-wildlife",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize all x \n",
    "# @warning each x are of different length\n",
    "x_tokenized = []\n",
    "for t in x:\n",
    "    x_tokenized.append(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(t)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banner-clear",
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passing-chess",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valuable-calibration",
   "metadata": {},
   "source": [
    "# Gif Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "settled-cream",
   "metadata": {},
   "source": [
    "train a subset of data with batch size 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charged-navigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "os.chdir('gifs')\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "talented-handbook",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = []\n",
    "for g in list(df['y']):\n",
    "    y.append(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spatial-barrel",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gif_downloader(image_urls, status=[], filenames = []):\n",
    "    \n",
    "    for index, img in enumerate(image_urls):\n",
    "        # We can split the file based upon / and extract the last split within the python list below:\n",
    "        file_name = img.split('/')[-1]\n",
    "        #print(\"fThis is the file name: {file_name}\")\n",
    "        filenames.append(file_name) \n",
    "        # Now let's send a request to the image URL:\n",
    "        r = requests.get(img, stream=True)\n",
    "        # We can check that the status code is 200 before doing anything else:\n",
    "        if r.status_code == 200:\n",
    "            # This command below will allow us to write the data to a file as binary:\n",
    "            with open(file_name, 'wb') as f:\n",
    "                for chunk in r:\n",
    "                    f.write(chunk)\n",
    "            status.append(True)\n",
    "        else:\n",
    "            # We will write all of the images back to the broken_images list:\n",
    "            status.append(False)\n",
    "    return filenames, status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pursuant-swift",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gif(image_path, sess):\n",
    "    #load gif using tf.io.readfile. make sure dir is correct if file not found\n",
    "    image = tf.io.read_file(image_path)\n",
    "    #use tf.decode to convert gif to a matrix\n",
    "    image = tf.image.decode_gif(image)\n",
    "    return sess.run(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continued-amateur",
   "metadata": {},
   "outputs": [],
   "source": [
    "q=[]\n",
    "filenames=[]\n",
    "filenames, q = gif_downloader(y[1:50], status=q)\n",
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transparent-cartoon",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = []\n",
    "for file in filenames:\n",
    "    with tf.compat.v1.Session() as sess:\n",
    "        curr = load_gif(file, sess)\n",
    "    tensor = tf.convert_to_tensor(\n",
    "        curr, dtype='float32', dtype_hint=None, name=None\n",
    "    )\n",
    "    y_train.append(tf.math.log(tf.math.reduce_mean(tf.image.total_variation(tensor))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "physical-vermont",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.compat.v1.Session() as sess:\n",
    "    orginial = load_gif('tumblr_msij5q4Run1qd76t9o1_500.gif', sess)\n",
    "\n",
    "    \n",
    "tensor = tf.convert_to_tensor(\n",
    "    orginial, dtype='float32', dtype_hint=None, name=None\n",
    ")\n",
    "\n",
    "print(orginial)#print original np array\n",
    "print(tensor) #print converted tensor \n",
    "#dim is (frame,h,w,channel)\n",
    "#print(tf.math.reduce_std(tensor, dtype=uint8))\n",
    "#compute the pixel noise of every frame and take the noise average \n",
    "print(tf.math.log(tf.math.reduce_mean(tf.image.total_variation(tensor))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yellow-vegetable",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_3_tensor = tf.constant([\n",
    "  [[0, 1, 2, 3, 4],\n",
    "   [5, 6, 7, 8, 9]],\n",
    "  [[10, 11, 12, 13, 14],\n",
    "   [15, 16, 17, 18, 19]],\n",
    "  [[20, 21, 22, 23, 24],\n",
    "   [25, 26, 27, 28, 29]],])\n",
    "\n",
    "#print(tf.reshape(rank_3_tensor, [-1]))\n",
    "print(tf.math.reduce_sum(\n",
    "    rank_3_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blank-spanish",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_tokenized[1:50] \n",
    "# x_train[2]\n",
    "print(y_train[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authentic-cable",
   "metadata": {},
   "source": [
    "# Training Data Final Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grateful-agency",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a copy of x_train and y_train\n",
    "#may not be actually working. Check python data mutability for insurancwe\n",
    "x_train_cp = x_train\n",
    "y_train_cp = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "white-facing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thorough-ministry",
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy.min(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hazardous-biotechnology",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "environmental-warner",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge x and y and add x len\n",
    "x_train_with_len = [[x, y_train_cp[i], len(x_train_cp)]\n",
    "                 for i, x in enumerate(x_train_cp)]\n",
    "x_train_with_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bright-transaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort dataset based on x len (the length of the tweet)\n",
    "x_train_with_len.sort(key=lambda x: x[2])\n",
    "sorted_data = [(data[0], data[1]) for data in x_train_with_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceramic-necessity",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert them into tensorflow dataset\n",
    "processed_dataset = tf.data.Dataset.from_generator(lambda: sorted_data, output_types=(tf.int32, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greater-ancient",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 10\n",
    "batched_dataset = processed_dataset.padded_batch(BATCH_SIZE, padded_shapes=((None, ), ()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hawaiian-duration",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(batched_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "centered-flower",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_BATCHES = math.ceil(len(sorted_data) / BATCH_SIZE)\n",
    "# TEST_BATCHES = TOTAL_BATCHES // 10\n",
    "TEST_BATCHES = 1\n",
    "batched_dataset.shuffle(TOTAL_BATCHES)\n",
    "test_data = batched_dataset.take(TEST_BATCHES)\n",
    "train_data = batched_dataset.skip(TEST_BATCHES)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scientific-devices",
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in test_data:\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boxed-floor",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bottom-costa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizer = preprocessing.Normalization(input_shape=[1,])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adopted-league",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TEXT_MODEL(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 vocabulary_size,\n",
    "                 embedding_dimensions=128,\n",
    "                 cnn_filters=50,\n",
    "                 dnn_units=512,\n",
    "                 model_output_classes=2,\n",
    "                 dropout_rate=0.1,\n",
    "                 training=False,\n",
    "                 name=\"text_model\"):\n",
    "        super(TEXT_MODEL, self).__init__(name=name)\n",
    "        \n",
    "        self.embedding = layers.Embedding(vocabulary_size,\n",
    "                                          embedding_dimensions)\n",
    "#         self.cnn_layer1 = layers.Conv1D(filters=cnn_filters,\n",
    "#                                          kernel_size=2,\n",
    "#                                          padding=\"valid\",\n",
    "#                                          activation=\"relu\")\n",
    "#         self.cnn_layer2 = layers.Conv1D(filters=cnn_filters,\n",
    "#                                          kernel_size=3,\n",
    "#                                          padding=\"valid\",\n",
    "#                                          activation=\"relu\")\n",
    "#         self.cnn_layer3 = layers.Conv1D(filters=cnn_filters,\n",
    "#                                          kernel_size=4,\n",
    "#                                          padding=\"valid\",\n",
    "#                                          activation=\"relu\")\n",
    "#         self.pool = layers.GlobalMaxPool1D()\n",
    "\n",
    "#         self.dense_1 = layers.Dense(units=dnn_units, activation=\"relu\")\n",
    "#         self.dropout = layers.Dropout(rate=dropout_rate)\n",
    "#         if model_output_classes == 2:\n",
    "        self.last_dense = layers.Dense(units = 1)\n",
    "#         self.last_dense = layers.Dense(units=1,\n",
    "#                                             activation=\"sigmoid\")\n",
    "#         else:\n",
    "#             self.last_dense = layers.Dense(units=model_output_classes,\n",
    "#                                            activation=\"softmax\")\n",
    "    \n",
    "    def call(self, inputs, training):\n",
    "        l = self.embedding(inputs)\n",
    "#         l_1 = self.cnn_layer1(l) \n",
    "#         l_1 = self.pool(l_1) \n",
    "#         l_2 = self.cnn_layer2(l) \n",
    "#         l_2 = self.pool(l_2)\n",
    "#         l_3 = self.cnn_layer3(l)\n",
    "#         l_3 = self.pool(l_3) \n",
    "        \n",
    "#         concatenated = tf.concat([l_1, l_2, l_3], axis=-1) # (batch_size, 3 * cnn_filters)\n",
    "#         concatenated = self.dense_1(concatenated)\n",
    "#         concatenated = self.dropout(concatenated, training)\n",
    "        model_output = self.last_dense(l)\n",
    "        \n",
    "        return model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formal-reception",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_LENGTH = len(tokenizer.vocab)\n",
    "EMB_DIM = 200\n",
    "CNN_FILTERS = 100\n",
    "DNN_UNITS = 256\n",
    "OUTPUT_CLASSES = 2\n",
    "DROPOUT_RATE = 0.2\n",
    "NB_EPOCHS = 20\n",
    "text_model = TEXT_MODEL(vocabulary_size=VOCAB_LENGTH,\n",
    "                        embedding_dimensions=EMB_DIM,\n",
    "                        cnn_filters=CNN_FILTERS,\n",
    "                        dnn_units=DNN_UNITS,\n",
    "                        model_output_classes=OUTPUT_CLASSES,\n",
    "                        dropout_rate=DROPOUT_RATE)\n",
    "text_model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.1), loss='mean_absolute_error',metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "backed-garlic",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = text_model.fit(train_data, epochs=NB_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offensive-fifteen",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_loss(history):\n",
    "    plt.plot(history.history['loss'], label='loss')\n",
    "    plt.ylim([0, 10])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Error')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polyphonic-circular",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = text_model.evaluate(test_data)\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beneficial-advantage",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "national-compression",
   "metadata": {},
   "source": [
    "# Approach 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "listed-pierre",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_batch, label_batch = next(iter(batched_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "temporal-differential",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_review, first_label = text_batch[0], label_batch[0]\n",
    "print(\"Review\", first_review)\n",
    "print(\"Label\", first_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooperative-junior",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_LENGTH = len(tokenizer.vocab)\n",
    "EMB_DIM = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lightweight-clark",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import losses\n",
    "\n",
    "#GlobalAveragePooling1D layer returns a fixed-length output vector for each example by averaging over the sequence dimension.\n",
    "#allows the model to handle input of variable length, in the simplest way possible\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Embedding(VOCAB_LENGTH + 1, EMB_DIM),\n",
    "    layers.Dropout(0.2),    \n",
    "    layers.GlobalAveragePooling1D(), \n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(1, activation='linear') #set activation to linear for regression\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# model.compile(loss=losses.BinaryCrossentropy(from_logits=True),\n",
    "#               optimizer='adam',\n",
    "#               metrics=tf.metrics.BinaryAccuracy(threshold=0.0))\n",
    "\n",
    "#model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.1), loss='mean_absolute_error',metrics=[\"accuracy\"])\n",
    "\n",
    "#ref for keras for regression\n",
    "#https://towardsdatascience.com/keras-101-a-simple-and-interpretable-neural-network-model-for-house-pricing-regression-31b1a77f05ae\n",
    "\n",
    "#ref for keras dense layer\n",
    "#https://medium.com/@hunterheidenreich/understanding-keras-dense-layers-2abadff9b990\n",
    "#https://keras.io/api/layers/core_layers/dense/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superb-sequence",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "history = model.fit(\n",
    "    train_data,\n",
    "    validation_data=test_data,\n",
    "    epochs=epochs)\n",
    "\n",
    "# history = model.fit(\n",
    "#     x_train,\n",
    "#     y_train,\n",
    "#     validation_split=0.05,\n",
    "#     epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extraordinary-heater",
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in train_data:\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "private-factory",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.data.Dataset.as_numpy(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "copyrighted-bowling",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install plotly\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "balanced-findings",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geographic-power",
   "metadata": {},
   "outputs": [],
   "source": [
    "(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sacred-skill",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scattergl(y=history.history['loss'],\n",
    "                    name='Train'))\n",
    "\n",
    "fig.add_trace(go.Scattergl(y=history.history['val_loss'],\n",
    "                     name='Valid'))\n",
    "\n",
    "fig.update_layout(height=500, width=700,\n",
    "                  xaxis_title='Epoch',\n",
    "                  yaxis_title='Loss')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desirable-think",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(test_data)\n",
    "\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designed-parent",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model \n",
    "\n",
    "import joblib\n",
    "model.save(\"/Users/zimingfang/Desktop/Animated GIFs/AwesomeGif/GIF models/pixel_noise_prediction_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adolescent-perry",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revised-croatia",
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "played-doubt",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_generator(\n",
    "    generator=itr_train, validation_data=itr_valid, validation_steps=batch_size,\n",
    "    epochs=epochs, steps_per_epoch=num_batches, callbacks=cbs, verbose=1, workers=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minute-nigeria",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
