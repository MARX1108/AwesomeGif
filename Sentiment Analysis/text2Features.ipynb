{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text2Features.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQcYFDrdoXRC"
      },
      "source": [
        "This Code Will Be Used To Create 6 Different Features For A Given Text\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TD_jKyDlodc1"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.models import Model\n",
        "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing import sequence\n",
        "from keras.utils import to_categorical\n",
        "from keras.callbacks import EarlyStopping\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "# Load model\n",
        "sarcasticModel = load_model('sarcasmModel.h5')\n",
        "sentimentModel = load_model('sentimentModel.h5')\n"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABPzMabr0arG"
      },
      "source": [
        "#Creating Tokenizers\n",
        "#Sarcasm\n",
        "sarcasmDF = pd.read_csv(\"Clean_Sarcasm_Reddit.csv\")\n",
        "for idx,row in sarcasmDF.iterrows():\n",
        "    row[1] = row[1].replace('rt',' ')\n",
        "    \n",
        "max_words = 2000\n",
        "sarcasticTokenizer = Tokenizer(num_words=max_words, split=' ')\n",
        "sarcasticTokenizer.fit_on_texts(sarcasmDF['headline'].values)\n",
        "#Sentiment \n",
        "sentimentDF = pd.read_csv(\"Twitter_Data_2.csv\")\n",
        "max_words = 5000\n",
        "max_len=50\n",
        "\n",
        "sentimentTokenizer = Tokenizer(num_words=max_words, lower=True, split=' ')\n",
        "\n",
        "sentimentTokenizer.fit_on_texts(sentimentDF['clean_text'])"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6mbhfnBpz5d"
      },
      "source": [
        "def predictSentiment(text):\n",
        "  '''Function to predict sentiment class of the passed text'''\n",
        "    \n",
        "  sentimentClasses = ['Negative', 'Neutral', 'Positive']\n",
        "  max_len=50\n",
        "    \n",
        "  # Transforms text to a sequence of integers using a tokenizer object\n",
        "  xt = sentimentTokenizer.texts_to_sequences(text)\n",
        "  # Pad sequences to the same length\n",
        "  xt = pad_sequences(xt, padding='post', maxlen=max_len)\n",
        "  # Do the prediction using the loaded model\n",
        "  yt = sentimentModel.predict(xt).argmax(axis=1)\n",
        "  # Print the predicted sentiment\n",
        "\n",
        "  return sentimentClasses[yt[0]]  "
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pmJ9kl7qAlQ"
      },
      "source": [
        "\n",
        "\n",
        "def predictSarcasm(text):\n",
        "  '''Function to predict sarcasm class of the passed text'''\n",
        "    \n",
        "  sarcasticClasses = ['Not-Sarcastic', 'Sarcastic']\n",
        "  max_len=50\n",
        "    \n",
        "  # Transforms text to a sequence of integers using a tokenizer object\n",
        "  xt = sarcasticTokenizer.texts_to_sequences(text)\n",
        "  print(xt)\n",
        "  # Pad sequences to the same length\n",
        "  xt = pad_sequences(xt, padding='post', maxlen=max_len)\n",
        "  # Do the prediction using the loaded model\n",
        "  print(xt)\n",
        "  yt = sarcasticModel.predict(xt).argmax(axis=1)\n",
        "  # Print the predicted sentiment\n",
        "  return sarcasticClasses[yt[0]] "
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dicmst3JyDID",
        "outputId": "ed4c9973-9ea3-42e9-f07a-4eea4ef73e28"
      },
      "source": [
        "!pip install text2emotion\n",
        "import text2emotion as te"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting text2emotion\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fe/31/b190e37c1396ca68ab1b5c8ea1a23f2f7848df532ad69133e94853120aed/text2emotion-0.0.5-py3-none-any.whl (57kB)\n",
            "\r\u001b[K     |█████▊                          | 10kB 15.5MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 20kB 11.7MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 30kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 40kB 7.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 51kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 3.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from text2emotion) (3.2.5)\n",
            "Collecting emoji>=0.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/24/fa/b3368f41b95a286f8d300e323449ab4e86b85334c2e0b477e94422b8ed0f/emoji-1.2.0-py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 6.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->text2emotion) (1.15.0)\n",
            "Installing collected packages: emoji, text2emotion\n",
            "Successfully installed emoji-1.2.0 text2emotion-0.0.5\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjlvzMLD48sJ"
      },
      "source": [
        "def convertTextToEmotions(text):\n",
        "    return te.get_emotion(text)\n",
        "    "
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Elj7TQqy5ZLn",
        "outputId": "31548daf-79dc-4490-c3b1-9369cc8e69eb"
      },
      "source": [
        "!pip install textstat"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting textstat\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/b1/ab40a00b727a0d209402d1be6aa3f1bc75bd03678b59ace8507b08bf12f5/textstat-0.7.0-py3-none-any.whl (99kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 3.6MB/s \n",
            "\u001b[?25hCollecting pyphen\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/5a/5bc036e01389bc6a6667a932bac3e388de6e7fa5777a6ff50e652f60ec79/Pyphen-0.10.0-py3-none-any.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 7.4MB/s \n",
            "\u001b[?25hInstalling collected packages: pyphen, textstat\n",
            "Successfully installed pyphen-0.10.0 textstat-0.7.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSFn9iQc5c8W"
      },
      "source": [
        "import textstat as ts"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Blweodwx5fUM"
      },
      "source": [
        "def convertTextToStats(text):\n",
        "  # get the text for the row and create the scores \n",
        "  readingEase = ts.flesch_reading_ease(text)\n",
        "  dcScore = ts.dale_chall_readability_score(text)\n",
        "  lFormula = ts.linsear_write_formula(text)\n",
        "  return {\"Reading Ease\": readingEase, \"Dale-Chall\": dcScore, \"Linsear Write\": lFormula}"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4fwkDiT5xfQ"
      },
      "source": [
        "def extractTextFeatures(text):\n",
        "  # Emotions dictionary\n",
        "  emotions = convertTextToEmotions(text)\n",
        "  # Stat dicitonary\n",
        "  stats = convertTextToStats(text)\n",
        "  # ML dictinoary \n",
        "  ml = {\"Sarcasm Indicator\": predictSarcasm([text]), \"Sentiment\": predictSentiment([text])} \n",
        "  return {**emotions,  **stats, **ml}"
      ],
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CeIW0rXR51is",
        "outputId": "0b586538-ee53-434d-84c8-eb05bc9f4e84"
      },
      "source": [
        "extractTextFeatures(\"I want to go on a walk today with my friends\")"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[65, 176, 1, 178, 7, 6, 1258, 477, 9, 78, 218]]\n",
            "[[  65  176    1  178    7    6 1258  477    9   78  218    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Angry': 0.0,\n",
              " 'Dale-Chall': 0.55,\n",
              " 'Fear': 0.0,\n",
              " 'Happy': 1.0,\n",
              " 'Linsear Write': 4.5,\n",
              " 'Reading Ease': 102.61,\n",
              " 'Sad': 0.0,\n",
              " 'Sarcasm Indicator': 'Sarcastic',\n",
              " 'Sentiment': 'Neutral',\n",
              " 'Surprise': 0.0}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YG3knJLc7oF-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}